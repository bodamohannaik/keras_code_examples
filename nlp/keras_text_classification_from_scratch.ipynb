{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_text_classification_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5Hn4a4zBrbO3dMfq1MaI0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bodamohannaik/keras_code_examples/blob/master/nlp/keras_text_classification_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "rpe8u_5sJq9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "k30tlN6CJzIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download IMDB movie reviews"
      ],
      "metadata": {
        "id": "4ktK85N1Kc6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
        "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
        "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
        "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
        "  month     = {June},\n",
        "  year      = {2011},\n",
        "  address   = {Portland, Oregon, USA},\n",
        "  publisher = {Association for Computational Linguistics},\n",
        "  pages     = {142--150},\n",
        "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
        "}"
      ],
      "metadata": {
        "id": "DAWY-4nnQN3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OiArVBWJ8Z9",
        "outputId": "7e7f124f-2d7c-435d-8486-aebb1c88d503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  29.3M      0  0:00:02  0:00:02 --:--:-- 29.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list down the subfolders\n",
        "!ls aclImdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksa2bN2yLsG9",
        "outputId": "fc980334-a9f0-4c06-b9b1-5795b6e7487b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/README"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srCrkpRdOToE",
        "outputId": "6d7a1c0c-878a-467f-e460-3e1d2b54e3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Movie Review Dataset v1.0\n",
            "\n",
            "Overview\n",
            "\n",
            "This dataset contains movie reviews along with their associated binary\n",
            "sentiment polarity labels. It is intended to serve as a benchmark for\n",
            "sentiment classification. This document outlines how the dataset was\n",
            "gathered, and how to use the files provided. \n",
            "\n",
            "Dataset \n",
            "\n",
            "The core dataset contains 50,000 reviews split evenly into 25k train\n",
            "and 25k test sets. The overall distribution of labels is balanced (25k\n",
            "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
            "documents for unsupervised learning. \n",
            "\n",
            "In the entire collection, no more than 30 reviews are allowed for any\n",
            "given movie because reviews for the same movie tend to have correlated\n",
            "ratings. Further, the train and test sets contain a disjoint set of\n",
            "movies, so no significant performance is obtained by memorizing\n",
            "movie-unique terms and their associated with observed labels.  In the\n",
            "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
            "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
            "more neutral ratings are not included in the train/test sets. In the\n",
            "unsupervised set, reviews of any rating are included and there are an\n",
            "even number of reviews > 5 and <= 5.\n",
            "\n",
            "Files\n",
            "\n",
            "There are two top-level directories [train/, test/] corresponding to\n",
            "the training and test sets. Each contains [pos/, neg/] directories for\n",
            "the reviews with binary labels positive and negative. Within these\n",
            "directories, reviews are stored in text files named following the\n",
            "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
            "the star rating for that review on a 1-10 scale. For example, the file\n",
            "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
            "example with unique id 200 and star rating 8/10 from IMDb. The\n",
            "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
            "omitted for this portion of the dataset.\n",
            "\n",
            "We also include the IMDb URLs for each review in a separate\n",
            "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
            "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
            "are unable to link directly to the review, but only to the movie's\n",
            "review page.\n",
            "\n",
            "In addition to the review text files, we include already-tokenized bag\n",
            "of words (BoW) features that were used in our experiments. These \n",
            "are stored in .feat files in the train/test directories. Each .feat\n",
            "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
            "data.  The feature indices in these files start from 0, and the text\n",
            "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
            "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
            "(the) appears 7 times in that review.\n",
            "\n",
            "LIBSVM page for details on .feat file format:\n",
            "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
            "\n",
            "We also include [imdbEr.txt] which contains the expected rating for\n",
            "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
            "rating is a good way to get a sense for the average polarity of a word\n",
            "in the dataset.\n",
            "\n",
            "Citing the dataset\n",
            "\n",
            "When using this dataset please cite our ACL 2011 paper which\n",
            "introduces it. This paper also contains classification results which\n",
            "you may want to compare against.\n",
            "\n",
            "\n",
            "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "  month     = {June},\n",
            "  year      = {2011},\n",
            "  address   = {Portland, Oregon, USA},\n",
            "  publisher = {Association for Computational Linguistics},\n",
            "  pages     = {142--150},\n",
            "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "}\n",
            "\n",
            "References\n",
            "\n",
            "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
            "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
            "636-659.\n",
            "\n",
            "Contact\n",
            "\n",
            "For questions/comments/corrections please contact Andrew Maas\n",
            "amaas@cs.stanford.edu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# delete other folder \n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "4Ueq_RFDXnKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load DataSet"
      ],
      "metadata": {
        "id": "ubcnQxZRV2am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = tf.keras.utils.text_dataset_from_directory('aclImdb/train', label_mode='binary', \n",
        "                                                           class_names=['neg', 'pos'], batch_size=32, subset=\"training\",\n",
        "                                                           validation_split =0.2, seed =0)\n",
        "dataset_val = tf.keras.utils.text_dataset_from_directory('aclImdb/train', label_mode='binary', \n",
        "                                                           class_names=['neg', 'pos'], batch_size=32, subset=\"validation\",\n",
        "                                                           validation_split =0.2, seed =0)\n",
        "dataset_test = tf.keras.utils.text_dataset_from_directory('aclImdb/test', label_mode='binary', \n",
        "                                                           class_names=['neg', 'pos'], batch_size=32,\n",
        "                                                            seed =0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K3S7_3qSqFz",
        "outputId": "82652c74-0bdd-4668-a67b-f3e5bf06f127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# view sample data\n",
        "for sample_reviews, labels in dataset_train.take(1):\n",
        "  for i in range(10):\n",
        "    print(\"-\"*80)\n",
        "    print(str(labels[i])+\":\"+sample_reviews[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkn45u86ewgd",
        "outputId": "5952ef90-4374-45df-c3a2-0c65c0cdcbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "tf.Tensor(b\"tf.Tensor([0.], shape=(1,), dtype=float32):Violence whether real or not always has an impact. In this film the violence is about as crass as you could ask for. In the Great Ecstacy the director has successfully demonstrated what extremes of violence people are capable of. But what was the point? The violence looks like a mix of No\\xc3\\xab's 'Irreversible, and ' Kubrick's 'Clockwork Orange'...both of which are remarkable films. Don't get me wrong, I'm not opposed to screen violence at all and I've seen some nasty stuff in my film-going years, but this film as a whole is totally juvenile. The story is never developed enough to offer any reason for the extreme violence, the rizla paper thin reason we are give for Robert's demise is his introduction to drugs. Danny Dyer plays the character who is partly responsible for Robert's drug fuelled demise, however he is on screen for less than 5 minutes. Lesley Manville is Robert's unable to cope mum, I am not sure what either of these actors is doing in a film of this low caliber. The acting is wooden, the scene in the kitchen with the TV-cook and his wife for instance is as painful to watch if not more so than the shocking finale- who wrote those dialogues?! Some of the comments the boys make...'looks like she's enjoying it' are so trite as to tempt one to laugh if it were not for Clay's ardent desire to bombard us with harrowing images of mutilated female genitals. Why we need to be shown such detail possibly down to the director's adolescent obsession with sadistic pornographic imagery...one can only wonder at this young man's psychology.<br /><br />The 'political meaning' of the film was repeatedly brought to our attention due to the amount of scenes; in the bar, outside the TV-cook's house, war in Iraq reports, was perhaps too obvious in my opinion. Yes, war is violent, social determinism causes frustration, we're all prone to horrifingly violent acts whether you're in politics or on the street popping E. Juxtaposing all these things as part of the same underlying issue is evading the actual issue which is the meaning of violence in man. This issue is one that we still haven't managed to grasp and certainly not in this film.<br /><br />My opinion: derivative, badly-made and pointless.\", shape=(), dtype=string)\n",
            "--------------------------------------------------------------------------------\n",
            "tf.Tensor(b'tf.Tensor([0.], shape=(1,), dtype=float32):In 1993, \"the visitors\" was an enormous hit in France. So, the sequence was inevitable and unfortunately, this sequence ranks among the worst ones ever made. <br /><br />This is a movie that doesn\\'t keep its promises. Indeed, it\\'s supposed to tell a sole story. Jean Reno must go in the twentieth century and take Christian Clavier back in the Middle Ages so that time can normally follow its course. The problem is that Clavier feels completely at ease in the world of the twentieth century, and so make him get back in the Middles Ages is rather hard... Instead of this, the movie goes on several other stories without succeeding in following the main plot. As a consequence, the movie becomes sometimes muddle-headed, sometimes a bit of a mess.<br /><br />But the movie also suffers from the performance of nearly all the actors. Reno and Clavier fall into the trap that however they could avoid in the first movie: they\\'re going over the top and become annoying. Then, why did Jean-Marie Poir\\xc3\\xa9 the film-maker engage Muriel Robin in the female main role? He made a mistake because she seems ill-at-ease and is absolutely pitiful. The other actors aren\\'t better: Marie-Anne Chazel is nonexistent and Christian Bujeau, unbearable.<br /><br /> Of course, the movie contains a few good moments with efficient gags but it often falls into vulgarity and easiness. Certain sequences and dialogs are affected. It also appears hollow because Poir\\xc3\\xa9 takes back elements that secured the success of the first movie. Thus, a young girl takes Reno for a close relative of her family and asks him to take part in her wedding.<br /><br />A labored and disappointing follow-up. Anyway, what\\'s the interest of this movie otherwise commercial?<br /><br />', shape=(), dtype=string)\n",
            "--------------------------------------------------------------------------------\n",
            "tf.Tensor(b\"tf.Tensor([0.], shape=(1,), dtype=float32):This movie gets both a 6/10 rating from me, as well as a 9/10. Here is why: As a standard horror movie for the standard horror crowd, where action and gore and scares are taken into consideration, this movie WILL bore you. It's basically a family drama similar to what you'd see on the Lifetime channel, but put in a horror universe. The story and formula are age-old, retreaded hundreds of times. If you're looking for any originality in the plot structure or the minimal conflicts, you'll be disappointed. Take away the zombies and you'll have something just as melodramatic as A Beautiful Mind, tripping on cheese. This is the 6/10.<br /><br />However, the basic synopsis and idea is pretty original and over-the-top. It's literally something you and your friends would joke about when you're half-drunk . . . but that joke actually got a theatrical release. The idea gets a 9/10 from me. The only reason it isn't perfect is because they could have taken it even further, but they didn't.<br /><br />The mix of both is mixed. I thought it was funny, but as with most all comedies, it wasn't THAT funny. I had my mom and little sister watch it with me and the jokes we made about it were funnier than the jokes scripted. There were moments of utter genius, but there were also moments of pure boredom.<br /><br />I sincerely hope that other movies take this kind of over-the-top risk and original ideas. I just can't say it was perfect, or even near it, because of the lack of originality to the plot.<br /><br />A GREAT family movie. A great movie to watch with a bunch of guys (or girls). A great movie to watch with anyone . . . but if you watch it alone, it will be a bit boring. Other people always make this kind of movie funnier and richer.<br /><br />4/10\", shape=(), dtype=string)\n",
            "--------------------------------------------------------------------------------\n",
            "tf.Tensor(b\"tf.Tensor([1.], shape=(1,), dtype=float32):I swear I could watch this movie every weekend of my life and never get sick of it! Every aspect of human emotion is captured so magically by the acting, the script, the direction, and the general feeling of this movie. It's been a long time since I saw a movie that actually made me choke from laughter, reflect from sadness, and feel each intended feeling that comes through in this most excellent work! We need MORE MOVIES like this!!! Mike Binder: are you listening???\", shape=(), dtype=string)\n",
            "--------------------------------------------------------------------------------\n",
            "tf.Tensor(b\"tf.Tensor([0.], shape=(1,), dtype=float32):Hammerhead is a combination between the mad scientist and killer shark movie genres. In a bit of type-casting, Jeffrey Combs plays the aforementioned mad scientist who develops a human/hammerhead shark creature. Bizarrely, this being is in fact his son, who he has turned into this monster to prevent him dying from cancer. Or something.<br /><br />A group of associates are invited to the scientist's private island. They end up being used as shark bait or shark mate. For some unknown reason the head of IT has been brought along as part of this team. Who knows why? Luckily, he turns out to be a resourceful, if somewhat overweight, Ramboesque hero. I'm working on the assumption that he learnt how to handle an assault rifle as part of his day job working in 1st line support. A normal day for this IT man presumably involves fixing someone's network connection followed by a call to gun down gun-toting evil-doers. Or perhaps a call to fix someone's PC has to be scheduled between physical confrontations with land-based human-shark hybrids? Anyway, he's amazing and saves the day. He even get's the girl.<br /><br />The shark-man is a slightly lame creation but OK, I guess, judging by the effects in general in this film. And the movie moves on at a decent pace. It's complete hokum of course but if you buy a movie called Hammerhead and expect it to be a complex drama about the emotional conflicts experienced by a man turned into a land-based killer fish, then really you have no one to blame but yourself. As it is, there are guns, gore, girls and possibly even an exploding helicopter. It's rubbish but not as bad as some might say.\", shape=(), dtype=string)\n",
            "--------------------------------------------------------------------------------\n",
            "tf.Tensor(b\"tf.Tensor([0.], shape=(1,), dtype=float32):I like bad movies. I like to rent bad movies with my friends and rip on them for their duration. Then there are abhorrent movies like this. Redline is not just a bad movie, but a telling sign that maybe the American movie industry should please, for the sake of the viewer, at least proofread scripts before funding a movie.<br /><br />If a stereotype took a crap, this movie would spawn from that. The storyline is unbearable, and the acting all around is laughable. Nadia Bjorlin and Eddie Griffin have, perhaps, the worst screen chemistry I've seen in a good while, and even individually they should be isolated from humanity and beaten with a bag of oranges until they change their profession to street merchants (about the only thing they can legitimately qualify for). Furthermore, how Angus Macfadyen got convinced to do this movie is so far beyond me that I can't even think of an analogy. I am a loyal fan of his, but this has made me question him.<br /><br />To sum it up. Several people want revenge for different reasons (and if you care enough to know what they are, you're a bigger person than me), so much so that it turns to violence (I guess). The movie is like Ouroboros, the snake that swallows its own tail, in that it's an endless cycle of confusion and dialogue not fit for human ears. This movie is essentially one big car commercial for the first half, and an indecipherable action movie for the rest, it should be avoided at any and all costs.<br /><br />I wish I could find one positive aspect to this movie, and I think it lies in the fact that eventually the credits do roll.<br /><br />P.S. Nadia Bjorlin, if that was YOU singing those two songs in this movie, then you are a hack, and I hope old age ravages you.<br /><br />P.S.S. If you DO rent this movie looking for a laughable experience, listen for the lyrics to Nadia Bjorlin's awesome songs.\", shape=(), dtype=string)\n",
            "--------------------------------------------------------------------------------\n",
            "tf.Tensor(b\"tf.Tensor([1.], shape=(1,), dtype=float32):I haven't really seen too many of the Columbo films... actually, I think I've only watched one or two, apart from this one. I've always liked Columbo, though, somehow without even having seen that much of him. Peter Falk is and has always been the perfect choice for the character, because of his looks, his voice and his charm. The perfect proof of this is that though the series started all the way back in 1968, the latest(and probably not last) of the films was made in 2003. That's 35 years. And Falk was 40 back when he made the first one. The series consists of 68 films(unless my count is off), all of which are made for TV. Everyone knows the character, even though no one has ever seen a film featuring him in the cinema. That is quite an accomplishment, if you ask me. The plot is pretty good. The only problem I have with it is that the killer and murder is revealed at the very beginning(though that may be the same for all of the Columbo films), leaving no mystery but how Columbo solves it, making it somewhat dull(since there's not much to look forward to at the end of the film). The pacing is good, there's hardly a scene where you're bored. The acting is very good, particularly that of Falk and Ruth Gordon. They have some great exchanges of dialog in the film. The characters are well-written and credible. The dialog and script is unusually good for a TV-movie. All in all, the film is, yes, surprisingly good for a TV-movie, and definitely worth watching for any fan of Columbo and/or crime/mystery flicks. 8/10\", shape=(), dtype=string)\n",
            "--------------------------------------------------------------------------------\n",
            "tf.Tensor(b'tf.Tensor([1.], shape=(1,), dtype=float32):Whenever I see most reviews it\\'s called \\'a misfire for Eddie Murphy\\'. These critics want to take a look at some of the stuff he\\'s doing these days, and maybe soften their stance in retrospect... \"The Golden Child\" is not highbrow entertainment, but thanks to some of the cast it breaths new life into old clich\\xc3\\xa9s, and gives Murphy one of his best roles. I don\\'t understand the pervading lack of \\'love\\' for its efforts, at all. Perhaps it was released at a time when the establishment had grown weary of knockabout, thrill-a-minute adventures? Steven Spielberg started it with Indiana Jones; it\\'s unfair to make this one a scapegoat when what is possibly its biggest sin is also utterly harmless. There\\'s nothing necessarily wrong with trying to capitalise on trends.<br /><br />Yes it\\'s silly, but even an occasional observer should be able to understand that \\'ridiculous\\' is where Hollywood\\'s idea of mysticism begins and ends. What\\'s more important than believability with a story like this is that the audience have entertaining tour guides on hand to show them the mysterious sights. Michael Ritchie and Eddie Murphy fit the bill for this capacity just fine. My advice to you is to buy the ticket and take the ride.', shape=(), dtype=string)\n",
            "--------------------------------------------------------------------------------\n",
            "tf.Tensor(b'tf.Tensor([1.], shape=(1,), dtype=float32):Having the opportunity to watch some of the filming in the Slavic Village-Broadway area I couldn\\'t wait to see it\\'s final copy.<br /><br />Viewing this film at the Cleveland Premier last Friday,I haven\\'t laughed out loud at a comedy in a long time! It is great slapstick. The Russo Brothers did a fine job directing. The entire cast performs their best comedic acting... No slow or dry segments... George Clooney is one of my favorite actors and he\\'s great as the crippled safe breaker in this flick. I was most imprest by William H. Macy as crook \"Riley\" and Michael Jeter\\'s as \"Toto\" they keep you in \"stitches\". I believe they have the funniest roles in the entire movie.', shape=(), dtype=string)\n",
            "--------------------------------------------------------------------------------\n",
            "tf.Tensor(b'tf.Tensor([0.], shape=(1,), dtype=float32):There was a Bugs Bunny cartoon titled \"Baby Buggy Bunny\" that was EXACTLY this plot. Baby-faced Finster robbed a bank and the money in the carriage rolled away and fell into Bug\\'s rabbit hole. He dressed up as a baby to get into Bugg\\'s hole to retrieve the money. The scene in \"Little Man\" where he\\'s looking in the bathroom mirror shaving with a cigar in his mouth is straight from the cartoon. This was a hilarious 5-minute cartoon; not so much an entire movie. If you are really interested in this, buy the Bugs Bunny DVD. It\\'s was much more original the first time (1954). Plus you\\'ll get a lot more classic Bugs Bunny cartoons to boot!', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization"
      ],
      "metadata": {
        "id": "pN9ghPbwqvXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardize(text):\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(input = text, pattern = \"<.+?>\", rewrite=\"\")\n",
        "  text = tf.strings.regex_replace(input = text, pattern =f\"[{re.escape(string.punctuation)}]\", rewrite=\"\")\n",
        "  return text"
      ],
      "metadata": {
        "id": "KUnyhCTDSDAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_standardize('the worst ones ever made. <br /><br />This is a movie that doesn\\'t keep its promises.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2X_ih8qTXSw",
        "outputId": "7118e937-3fd0-4ccb-9862-e6d7ce70bcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'the worst ones ever made this is a movie that doesnt keep its promises'>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary size\n",
        "max_features = 20000\n",
        "# max review length\n",
        "output_sequence_length = 500\n",
        "vectorization_layer = tf.keras.layers.TextVectorization(max_tokens=max_features, standardize=custom_standardize,\n",
        "                                                        split='whitespace',output_mode='int', output_sequence_length=output_sequence_length)\n",
        "vectorization_layer.adapt(dataset_train.map(lambda texts_temp, labels_temp: texts_temp))"
      ],
      "metadata": {
        "id": "p68cmz-Fj6GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"vocabulary size: {vectorization_layer.vocabulary_size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHD-FpuxRwrM",
        "outputId": "080eb03f-66b2-4f03-e8d9-45be12b95319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary size: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization_layer(['the worst ones ever made. <br /><br />This is a movie that doesn\\'t keep its promises.'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1gxjgdtnBrk",
        "outputId": "073d56e4-7bb1-4e4f-9e04-5fc6995a98cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 500), dtype=int64, numpy=\n",
              "array([[   2,  239,  524,  121,   90,   10,    7,    4,   17,   12,  144,\n",
              "         379,   29, 4528,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model"
      ],
      "metadata": {
        "id": "yTHKmxk1qU4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input layer\n",
        "inputs = tf.keras.layers.Input(shape=(1,),dtype = tf.string)\n",
        "# vectorization\n",
        "x = vectorization_layer(inputs)\n",
        "# embedding layer\n",
        "x = tf.keras.layers.Embedding(input_dim=max_features, output_dim=128)(x)\n",
        "# Convolution layer\n",
        "x = tf.keras.layers.Conv1D(filters = 64, kernel_size=7, padding=\"valid\", strides = 3, activation ='relu')(x)\n",
        "x = tf.keras.layers.Conv1D(filters = 32, kernel_size=7, padding =\"valid\", strides =3, activation = 'relu')(x)\n",
        "x = tf.keras.layers.Dropout(rate = .5)(x)\n",
        "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "# Dense layer\n",
        "x = tf.keras.layers.Dense(units=128, activation ='relu')(x)\n",
        "x = tf.keras.layers.Dropout(rate =.5)(x)\n",
        "x = tf.keras.layers.Dense(units = 1, activation = 'sigmoid')(x)\n",
        "# model\n",
        "model = tf.keras.Model(inputs = inputs, outputs = x)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "bB4bXbvVp-J1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca57ac38-d95b-4cc0-ea72-6225bee4ffd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVec  (None, 500)              0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding_7 (Embedding)     (None, 500, 128)          2560000   \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 165, 64)           57408     \n",
            "                                                                 \n",
            " conv1d_12 (Conv1D)          (None, 53, 32)            14368     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 53, 32)            0         \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 32)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               4224      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,636,129\n",
            "Trainable params: 2,636,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(dataset_train, epochs = 10, validation_data = dataset_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82at73Ps_hYx",
        "outputId": "0fc66fc0-97eb-4122-f534-f54588740dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 74s 116ms/step - loss: 0.4861 - accuracy: 0.7250\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 71s 113ms/step - loss: 0.2003 - accuracy: 0.9220\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 71s 113ms/step - loss: 0.0877 - accuracy: 0.9700\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 72s 114ms/step - loss: 0.0355 - accuracy: 0.9876\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 71s 113ms/step - loss: 0.0216 - accuracy: 0.9925\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 71s 114ms/step - loss: 0.0163 - accuracy: 0.9947\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 72s 114ms/step - loss: 0.0162 - accuracy: 0.9940\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 83s 133ms/step - loss: 0.0161 - accuracy: 0.9940\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 74s 118ms/step - loss: 0.0102 - accuracy: 0.9968\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 73s 117ms/step - loss: 0.0111 - accuracy: 0.9963\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd976b2d50>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i51ZMi2IBrvO",
        "outputId": "64f004d6-f164-4ea9-cbc5-78f26771dc24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 23s 29ms/step - loss: 0.7486 - accuracy: 0.8208\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7486086487770081, 0.8207600116729736]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}